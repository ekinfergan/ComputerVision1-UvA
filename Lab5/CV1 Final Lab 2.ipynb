{"cells":[{"cell_type":"markdown","metadata":{"id":"d7PRKfXw99hy"},"source":["<center><img src=\"https://editor.analyticsvidhya.com/uploads/46151Deep_Learning_Software_DE_1380x735px_1150x_.png\" width=50% ></center>\n","\n","# <center> Lab Project Part 2: Image Classification using Convolutional Neural Networks </center>\n","<center> Computer Vision 1, University of Amsterdam </center>\n","<center> Due 23:59, October 21, 2023 (Amsterdam time) </center>\n","\n","***\n","\n","<center>\n","<b>TA's: Xiaoyan Xing, Vladimir Yugay, Luca Pantea</b>\n","\n","Student1 ID: 15216608\\\n","Student1 Name: Despoina Touska\n","\n","Student2 ID: 12652954 \\\n","Student2 Name: Ekin Fergan\n","\n","Student3 ID: 14804557\\\n","Student3 Name: Gregory Hok Tjoan Go\n","\n","Student4 ID: 15194817\\\n","Student4 Name: Jesse Wiers\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"u3TPoO70b2iE"},"source":["# **Instructions**\n","\n","1. Students are expected to prepare a report covering both sections of this assignment. The report should include answers to all questions, written details on implementation approaches, the analysis of the results for different settings and visualizations to illustrate experiments with and performance of your implementation. Grading will primarily be based on the report (i.e. it should be self-contained as much as possible). If the report contains any faulty results or ambiguities, the TA's can take a look at your code to find out what happened.\n","\n","2. Do not just provide numbers without explanation, remember to follow the general guidelines and discuss different settings to show you understand the material and the processes at work.\n","\n","3. This part of the Final Lab contains an *optional* bonus challenge. See section 3 below.\n","\n","**Hint:** Having visual elements such as charts, graphs and plots are always useful for everyone. Keep this in mind while writing your reports.\n"]},{"cell_type":"markdown","metadata":{"id":"iY6wdmc299h1"},"source":["# **General Guidelines**\n","1. **Aim**:\n","     - Able to understand Image Recognition/Classification using Convolutional Neural Networks.\n","     - Get familiar with PyTorch, widely used Deep Learning framework\n","2. **Prerequisites**:\n","     - Familiarity with Python and relevant packages.\n","     - Know the basics of feature descriptors (SIFT, HoG) and machine learning tools (K-means, SVM and etc.).\n","3. **Guidelines**:\n","    Students should work on the assignments in their assignment group for **two** weeks.\n","\n","    Any questions regarding the assignment content can be discussed on Piazza.\n","    \n","    Your source code and report must be handed in together in a zip file (**ID1_ID2_ID3_part2.zip**) before the deadline. Make sure your report follows these guidelines:\n","    - *The maximum number of pages for this part is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n","    - *Follow the given instructions and answer all given questions. **Briefly describe what you implemented for each question in the report**.*\n","    - *Show you understand the algorithms and implementations: explain why certain settings produce certain results. When constructing graphs, tables and other figures, make your figures as informative as possible (choose relevant sample sizes, axes, etc.), to illustrate your arguments*\n","    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n","\n","4. The report should be handed in in **PDF-format**. Your code should be handed in in **.ipynb format** (Jupyter Notebook). This does not mean you have to make your project in a notebook, it just means it **should be submitted as a notebook**. This means that you can, for example, have separate python files in which you declare your classes/methods, but **_you will have to initialize/call these in the final notebook before you submit_**. Be sure to test whether all your functionality works as expected when ran in a notebook (**Before submission, go in the `Kernel` tab and press `Restart & Run All`**). If you use a Conda environment, be sure to include it in your submission.\n","\n","5. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n","\n","6. **Plagiarism note**: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of ChatGPT and other generative AI tools.\n","\n","<!-- ### PyTorch versions\n","we assume that you are using latest PyTorch version(>=1.4)\n","\n","### PyTorch Tutorial & Docs\n","This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n","\n","**Anaconda Environment** We recommend installing \\textit{anaconda} for configuring \\textit{python} package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n","\n","**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n","\n","**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n","\n","**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***:} What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n","You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"]},{"cell_type":"markdown","metadata":{"id":"D2EILNe3JR1b"},"source":["#  **Introduction**\n","\n","This part of the assignment makes use of Convolutional Neural Networks (CNN's). The previous part makes use of hand-crafted features like SIFT to represent images, then trains a classifier on top of them. In this way, learning is a two-step procedure with image representation and learning. The method used here instead *learns* the features jointly with the classification. Training CNNs roughly consists of three parts:\n","1. Creating the network architecture\n","\n","2. Preprocessing the data\n","\n","3. Feeding the data to the network, and updating the parameters.\n","\n","Please follow the instructions and finish the below tasks. (**Note:**  you are allowed to change the provided code.)"]},{"cell_type":"markdown","metadata":{"id":"CXQBnhT499h2"},"source":["# **Section 1: Image Classifiation on CIFAR-100**\n","### 1.1 Install pytorch and run the given code\n","\n","First of all, you need to install PyTorch and relevant packages. In this part, we will use [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) as the training and testing dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47K1Kzph99h3"},"outputs":[],"source":["#####################################################\n","# referenced code: https://pytorch.org/tutorials/\n","# referenced code: http://cs231n.stanford.edu/\n","# referenced code: https://cs.stanford.edu/~acoates/stl10/\n","######################################################\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.optim as optim\n","from PIL import Image\n","\n","from torch.utils.data import DataLoader, random_split, Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AU03smp99h4"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n","                                       download=False, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('apple', 'aquarium_fish', 'baby','bear', 'beaver','bed','bee','beetle','bicycle','bottle', 'bowl','boy','bridge', 'bus','butterfly', 'camel','can','castle','caterpillar','cattle',\n"," 'chair','chimpanzee','clock','cloud', 'cockroach','couch', 'cra','crocodile', 'cup','dinosaur','dolphin', 'elephant','flatfish', 'forest', 'fox','girl', 'hamster', 'house','kangaroo','keyboard',\n","'lamp', 'lawn_mower', 'leopard', 'lion','lizard','lobster', 'man','maple_tree','motorcycle', 'mountain', 'mouse','mushroom','oak_tree', 'orange','orchid', 'otter', 'palm_tree','pear', 'pickup_truck','pine_tree',\n","'plain', 'plate', 'poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea', 'seal', 'shark','shrew', 'skunk','skyscraper', 'snail','snake','spider',\n","'squirrel', 'streetcar', 'sunflower','sweet_pepper', 'table','tank','telephone', 'television', 'tiger','tractor','train','trout', 'tulip', 'turtle','wardrobe', 'whale', 'willow_tree','wolf', 'woman','worm')"]},{"cell_type":"markdown","metadata":{"id":"qvNbVoMh99h5"},"source":["####  **` Q1.1: Test dataloader and show the images of each class  of CIFAR-100 (3-pts)`**  \n","You need to run and modify the given code and **show** the example images of CIFAR-100, **describe** the classes and images of CIFAR-100. (Please visualize at least one picture for the classes of labels from 0 to 4.) (3-*pts*)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hhMU6z999h6"},"outputs":[],"source":["def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iPwTYDB99h7"},"outputs":[],"source":["# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(5)))"]},{"cell_type":"markdown","metadata":{"id":"DZ6OkCV9LIM9"},"source":["### 1.2 Architecture understanding\n","\n","In this section, we provide templates for two classes inheriting from the PyTorch superclass [*nn.Module*](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) (the base class for all neural network modules). One is an ordinary two-layer network (*TwolayerNet*) with fully connected layers and ReLu, and the other is a Convolutional Network (*ConvNet*) utilizing the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791).\n","\n","**Note**: you are allowed to change the provided function definitions."]},{"cell_type":"markdown","metadata":{"id":"w6a1z2vK99h7"},"source":["####  **`Q1.2: Architecture understanding. Implement architecture of TwolayerNet and ConvNet (4-pts).`**\n","\n","1. Complement the architecture of *TwolayerNet* class, and complement the architecture of *ConvNet* class using the structure of LeNet-5. (2-*pts*)\n","2. Since you need to feed color images into these two networks, what's the kernel size of the first convolutional layer in *ConvNet*? and how many trainable parameters are there in \"F6\" layer (given the calculation process)? (2-*pts*)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILrfnDDv99h8"},"outputs":[],"source":["class TwoLayerNet(nn.Module):\n","    # assign layer objects to class attributes\n","    # nn.init package contains convenient initialization methods\n","    # http://pytorch.org/docs/master/nn.html#torch-nn-init\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        '''\n","        :param input_size: 3*32*32\n","        :param hidden_size:\n","        :param num_classes:\n","        '''\n","        super(TwoLayerNet, self).__init__()\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","    def forward(self,x):\n","        x = x.view(x.shape[0], -1)\n","        scores = self.fc2(F.relu(self.fc1(x)))\n","        return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5E2a9-lz99h8"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","    def forward(self, x):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"aCP1tS7lL8hF"},"source":["### 1.3 Preparation of training\n","\n","In above section, we use the *CIFAR-100* dataset class from *torchvision.utils* provided by PyTorch. Whereas in most cases, you need to prepare the dataset yourself. One of the ways is to create a *dataset* class yourself and then use the *DataLoader* to make it iterable. After preparing the training and testing data, you also need to define the transform function for data augmentation and optimizer for parameter updating."]},{"cell_type":"markdown","metadata":{"id":"1oyc8Ksd99h-"},"source":["####  **` Q1.3: Preparation of training. Create the Dataloader class, define the transform function and the optimizer.(8-pts)`**  \n","1. Complement the *CIFAR100\\_loader* (2-pts)\n","2. Complement the *Transform* function and the *Optimizer* (2-pts)\n","3. Train the *TwolayerNet* and *ConvNet* with *CIFAR100\\_loader*, *Transform* and *Optimizer* you implemented and compare the results (4-pts)"]},{"cell_type":"markdown","metadata":{"id":"QkImoAsU99h-"},"source":["##### *` Complement the CIFAR100_loader class (2-pts)`*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9Za4VSP99h_"},"outputs":[],"source":["###  suggested reference: https://pytorch.org/tutorials/\n","# recipes/recipes/custom_dataset_transforms_loader.html?highlight=dataloader\n","# functions to show an image\n","\n","class CIFAR100_loader(torch.utils.data.Dataset):\n","    def __init__(self, root, train=True, transform=None):\n","        '''CIFAR-100 dataset loader'''\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, item):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","        return img, target"]},{"cell_type":"markdown","metadata":{"id":"mYKtNWW_99h_"},"source":["##### *` Complement the Transform function and the Optimizer (2-pts)`*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2yNqZpZ99h_"},"outputs":[],"source":["transform_train = ...\n","\n","transform_test = ...\n","\n","optimizer = ..."]},{"cell_type":"markdown","metadata":{"id":"EYpJN5Cb99h_"},"source":["##### *` Train the TwolayerNet and ConvNet with CIFAR100_loader, transform and optimizer you implemented and compare the results (4-pts)`*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FBozCDk99h-"},"outputs":[],"source":["def valid(net, testloader):\n","    net.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (accuracy))\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm7FuYwJ99h-"},"outputs":[],"source":["def valid_class(net, testloader, classes):\n","    net.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    class_correct = list(0. for _ in range(len(classes)))\n","    class_total = list(0. for _ in range(len(classes)))\n","    with torch.no_grad():\n","        for data in testloader:\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            c = (predicted == labels).squeeze()\n","            for i in range(len(labels)):\n","                label = labels[i]\n","                class_correct[label] += c[i].item()\n","                class_total[label] += 1\n","\n","    for i in range(len(classes)):\n","        print('Accuracy of %5s : %2d %%' % (\n","            classes[i], 100 * class_correct[i] / class_total[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNwBZvfX99h_"},"outputs":[],"source":["def train(net, train_loader, epochs=100):\n","    # Determine the device to use\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f'Training on {device}')\n","\n","    # Move the network to the device\n","    net.to(device)\n","\n","    ################################\n","    # Todo: finish the code\n","    ################################\n","\n","    # Training\n","    net.train()\n","    for epoch in range(epochs):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","    print('Finished Training')"]},{"cell_type":"markdown","metadata":{"id":"lCDHGe9-3bng"},"source":["*Initialize the datasets*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvF76gJx3bnh"},"outputs":[],"source":["# Datasets\n","train_set = CIFAR100_loader('./data', True, transform_train)\n","test_set = CIFAR100_loader('./data', False, transform_test)\n","\n","# Initialize the dataloaders\n","train_data_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n","test_data_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"wxfxa_Rj8X5z"},"source":["*Train the TwolayerNet network*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rd3ql4B27yEl"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"GySb8UWX8emz"},"source":["*Train the ConvNet network*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JL9by9A7yQb"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"SQhUzRuQOOSm"},"source":["### 1.4 Setting up the hyperparameters\n","\n","Some parameters must be set properly before the training of CNNs. These parameters shape the training procedure. They determine how many images are to be processed at each step, how much the weights of the network will be updated, how many iterations will the network run until convergence.  These parameters are called hyperparameters in the machine learning literature."]},{"cell_type":"markdown","metadata":{"id":"mhtGVNHIQ8kT"},"source":["####  **` Q1.4: Setting up the hyperparameters (10-pts)`**  \n","\n","1. Play with ConvNet and TwolayerNet yourself, set up the hyperparameters, and reach the accuracy as high as you can.\n","You can modify the *train*,  *Dataloader*, *transform* and *Optimizer* function as you like.\n","2. You can also modify the architectures of these two Nets. *Let's add 2 more layers in TwolayerNet and ConvNet, and show the results. (You can decide the size of these layers and where to add them.) Will you get higher performances? explain why.*\n","3.  Show the final results and described what you've done to improve the results. Describe and explain the influence of hyperparameters among *TwolayerNet* and *ConvNet*.\n","4. Compare and explain the differences of these two networks regarding the architecture, performances, and learning rates.\n","\n","**Hint:** You can adjust the following parameters and other parameters not listed as you like: *Learning rate, Batch size, Number of epochs, Optimizer, Transform function, Weight decay etc.* You can also change the structure a bit, for instance, adding Batch Normalization layers.\n","\n","**Note:** Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers."]},{"cell_type":"markdown","metadata":{"id":"p1Puiytn99h_"},"source":["#### *`Play with convNet and TwolayerNet, set up the hyperparameters and reach the accuracy as high as you can`*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROyqHC0199iA"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"pqQ6-Krp99iA"},"source":["#### *` Test the accuracy of ConvNet `*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciidRXDn99iA"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"mlAIR45199iA"},"source":["#### *`Test the accuracy of TwolayerNet`*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2fMEW-g99iA"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"3XX8c2fb99iA"},"source":["## **Section 2:  Finetuning the ConvNet**\n","### 2.1 STL-10 DATASET\n","> The above networks are trained on CIFAR-100, which\n","contains the images of 100 different object categories, each of which has $32\\times32 \\times3$ dimensions.\n","The dataset we use throughout this section is a subset of [STL-10](https://cs.stanford.edu/~acoates/stl10/)\n","with higher resolution and different object classes. So, there is a discrepancy between the previous dataset (CIFAR-100) and the new dataset (STL-10). One solution would be to train the whole network from scratch. However, the number of parameters is too large to be trained properly with such few images. Another way is to use the pre-trained network (on CIFAR-100) and then finetune the network on the new dataset (STL-10) (*e.g.*, use the same architectures in all layers except the output layer, as the number of output classes changes (from 100 to 5)).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XR1xyOW_sib8"},"outputs":[],"source":["# Use the following code if necessary\n","# referenced code: https://cs.stanford.edu/~acoates/stl10/\n","\n","from __future__ import print_function\n","\n","import sys\n","import os, sys, tarfile, errno\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","if sys.version_info >= (3, 0, 0):\n","    import urllib.request as urllib\n","else:\n","    import urllib\n","\n","try:\n","    from imageio import imsave\n","except:\n","    from scipy.misc import imsave\n","\n","print(sys.version_info)\n","\n","# image shape\n","HEIGHT = 96\n","WIDTH = 96\n","DEPTH = 3\n","\n","# size of a single image in bytes\n","SIZE = HEIGHT * WIDTH * DEPTH\n","\n","# path to the directory with the data\n","DATA_DIR = './data'\n","\n","# url of the binary data\n","DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n","\n","# path to the binary train file with image data\n","DATA_PATH = './data/stl10_binary/train_X.bin'\n","\n","# path to the binary train file with labels\n","LABEL_PATH = './data/stl10_binary/train_y.bin'\n","\n","def read_labels(path_to_labels):\n","    \"\"\"\n","    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n","    :return: an array containing the labels\n","    \"\"\"\n","    with open(path_to_labels, 'rb') as f:\n","        labels = np.fromfile(f, dtype=np.uint8)\n","        return labels\n","\n","\n","def read_all_images(path_to_data):\n","    \"\"\"\n","    :param path_to_data: the file containing the binary images from the STL-10 dataset\n","    :return: an array containing all the images\n","    \"\"\"\n","\n","    with open(path_to_data, 'rb') as f:\n","        # read whole file in uint8 chunks\n","        everything = np.fromfile(f, dtype=np.uint8)\n","\n","        # We force the data into 3x96x96 chunks, since the\n","        # images are stored in \"column-major order\", meaning\n","        # that \"the first 96*96 values are the red channel,\n","        # the next 96*96 are green, and the last are blue.\"\n","        # The -1 is since the size of the pictures depends\n","        # on the input file, and this way numpy determines\n","        # the size on its own.\n","\n","        images = np.reshape(everything, (-1, 3, 96, 96))\n","\n","        # Now transpose the images into a standard image format\n","        # readable by, for example, matplotlib.imshow\n","        # You might want to comment this line or reverse the shuffle\n","        # if you will use a learning algorithm like CNN, since they like\n","        # their channels separated.\n","        images = np.transpose(images, (0, 3, 2, 1))\n","        return images\n","\n","\n","def read_single_image(image_file):\n","    \"\"\"\n","    CAREFUL! - this method uses a file as input instead of the path - so the\n","    position of the reader will be remembered outside of context of this method.\n","    :param image_file: the open file containing the images\n","    :return: a single image\n","    \"\"\"\n","    # read a single image, count determines the number of uint8's to read\n","    image = np.fromfile(image_file, dtype=np.uint8, count=SIZE)\n","    # force into image matrix\n","    image = np.reshape(image, (3, 96, 96))\n","    # transpose to standard format\n","    # You might want to comment this line or reverse the shuffle\n","    # if you will use a learning algorithm like CNN, since they like\n","    # their channels separated.\n","    image = np.transpose(image, (2, 1, 0))\n","    return image\n","\n","\n","def plot_image(image):\n","    \"\"\"\n","    :param image: the image to be plotted in a 3-D matrix format\n","    :return: None\n","    \"\"\"\n","    plt.imshow(image)\n","    plt.show()\n","\n","def save_image(image, name):\n","    imsave(\"%s.png\" % name, image, format=\"png\")\n","\n","def download_and_extract():\n","    \"\"\"\n","    Download and extract the STL-10 dataset\n","    :return: None\n","    \"\"\"\n","    dest_directory = DATA_DIR\n","    if not os.path.exists(dest_directory):\n","        os.makedirs(dest_directory)\n","    filename = DATA_URL.split('/')[-1]\n","    filepath = os.path.join(dest_directory, filename)\n","    if not os.path.exists(filepath):\n","        def _progress(count, block_size, total_size):\n","            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n","                float(count * block_size) / float(total_size) * 100.0))\n","            sys.stdout.flush()\n","        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n","        print('Downloaded', filename)\n","        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n","\n","def save_images(images, labels):\n","    print(\"Saving images to disk\")\n","    i = 0\n","    for image in images:\n","        label = labels[i]\n","        directory = './img/' + str(label) + '/'\n","        try:\n","            os.makedirs(directory, exist_ok=True)\n","        except OSError as exc:\n","            if exc.errno == errno.EEXIST:\n","                pass\n","        filename = directory + str(i)\n","        print(filename)\n","        save_image(image, filename)\n","        i = i + 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU0aikuks2Lp"},"outputs":[],"source":["# Use the following code if necessary\n","\n","# download data if needed\n","download_and_extract()\n","\n","# test to check if the image is read correctly\n","with open(DATA_PATH) as f:\n","    image = read_single_image(f)\n","    plot_image(image)\n","\n","# test to check if the whole dataset is read correctly\n","images = read_all_images(DATA_PATH)\n","print(images.shape)\n","\n","labels = read_labels(LABEL_PATH)\n","print(labels.shape)\n","\n","# save images to disk\n","save_images(images, labels)"]},{"cell_type":"markdown","metadata":{"id":"pW0p0R6d99iA"},"source":["#### **`Q2.1 Create the STL10_Dataset (5-pts)`**\n","In this Section, download STL-10 and extract 5 classes from STL-10 training dataset. The the labels of images will be defined as:\n","\n","`{1: 'car', 2:'deer', 3:'horse', 4:'monkey', 5:'truck'}`\n","\n"," Extract mentioned 5 classes of images from STL-10. Complement *`STL10_Dataset`* class and match each class with the label accordingly. __Hint__: You can use the code above to help to complement *`STL10_Dataset`* class. (5-pts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPhepFAq99iA"},"outputs":[],"source":["class STL10_Dataset(Dataset):\n","    def __init__(self, root, train=True, transform=None):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","\n","    def __len__(self):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","    def __getitem__(self, item):\n","        ################################\n","        # Todo: finish the code\n","        ################################\n","\n","        return img, target\n"]},{"cell_type":"markdown","metadata":{"id":"pNLyFVEWw3Ge"},"source":["### 2.2 Fine-tuning ConvNet\n","You should load the pre-trained parameters and modify the output layer of pre-trained ConvNet from 100 to 5. You can either load the pre-trained parameters and then modify the output layer, or change the output layer firstly and then load the matched pre-trained parameters. The examples can be found at [link1](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and [link2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n"]},{"cell_type":"markdown","metadata":{"id":"AC3ipaYR99iB"},"source":["#### **`Q2.2  Finetuning from ConvNet (10-pts)`**\n","1. Load the pre-trained parameters (pretrained on CIFAR-100) and modify the ConvNet. (5-pts)\n","2. Train the model and show the results (settings of hyperparameters, accuracy, learning curve). (5-pts)\n","\n","**Hint**:  Once the network is trained, it is a good practice to understand the feature space by visualization techniques. There are several techniques to visualize the feature space. [**t-sne**](https://lvdmaaten.github.io/tsne/) is a dimensionality reduction method which can help you better understand the feature learning process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tp94zmaw3bnl","scrolled":true},"outputs":[],"source":["################################\n","# Load the pre-trained parameters (pretrained on CIFAR-100) and modify the ConvNet. (5-pts)\n","# Todo: finish the code\n","################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PN_l_WX199iB","scrolled":true},"outputs":[],"source":["################################\n","# Train the model and show the results (settings of hyperparameters, accuracy, learning curve). (5-pts)\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"nDUT9jtY99iB"},"source":["#  **3. Bonus (optional)**\n","Play with your code and try to get a higher accuracy on the test dataset (5 class from STL-10), as high as you can. The teams with the highest accuracy will get extra points, that will go on top of their average lab grade (weighed sum of all lab assignments) (your final grade can not exceed 10):\n","\n","**1st place:** *5%*\n","\n","**2nd and 3rd place:** *4%*\n","\n","**4th and 5th place:** *3%*\n","\n","**6th and 7th place:** *2%*\n","\n","**8th-10th place:** *1%*.\n","\n","You can adjust the hyperparameters and changing structures. Your strategies should be described and explained in your report.\n","\n","**Note:** Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers.\n","\n","**Note:** The only data you can use is from CIFAR-100 and SLT-10.\n","\n","**Hints**:\n","*   Data augmentation\n","*   Grid Search\n","*   Freezing early layers\n","*   Modifying Architecture\n","*   Modifying hyperparameters, *etc*.\n","*   [Other advice](https://cs231n.github.io/transfer-learning/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1YsjfP199iB"},"outputs":[],"source":["################################\n","# Todo: finish the code\n","################################"]},{"cell_type":"markdown","metadata":{"id":"grbC046Fgcs0"},"source":[" # Individual Contribution Report *(Mandatory)*\n","\n","Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."]},{"cell_type":"markdown","metadata":{"id":"_I5rcHbegcs0"},"source":["Name: Despoina Touska\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","metadata":{"id":"CkrptmyK5n4U"},"source":["Name: Ekin Fergan\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","metadata":{"id":"iZ7N7CXO5n68"},"source":["Name: Gregory Hok Tjoan Go\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","metadata":{"id":"WLFxLJZg5oPk"},"source":["Name: Jesse Wiers\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","metadata":{"id":"RctQ8Z1Cgcs0"},"source":[" # -End of Notebook-"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cv1","language":"python","name":"cv1"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
