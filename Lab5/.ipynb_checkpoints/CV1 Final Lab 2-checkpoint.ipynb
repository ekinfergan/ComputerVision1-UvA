{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PRKfXw99hy"
   },
   "source": [
    "<center><img src=\"https://editor.analyticsvidhya.com/uploads/46151Deep_Learning_Software_DE_1380x735px_1150x_.png\" width=50% ></center>\n",
    "\n",
    "# <center> Lab Project Part 2: Image Classification using Convolutional Neural Networks </center>\n",
    "<center> Computer Vision 1, University of Amsterdam </center>\n",
    "<center> Due 23:59, October 21, 2023 (Amsterdam time) </center>\n",
    "\n",
    "***\n",
    "\n",
    "<center>\n",
    "<b>TA's: Xiaoyan Xing, Vladimir Yugay, Luca Pantea</b>\n",
    "\n",
    "Student1 ID: 15216608\\\n",
    "Student1 Name: Despoina Touska\n",
    "\n",
    "Student2 ID: 12652954 \\\n",
    "Student2 Name: Ekin Fergan\n",
    "\n",
    "Student3 ID: 14804557\\\n",
    "Student3 Name: Gregory Hok Tjoan Go\n",
    "\n",
    "Student4 ID: 15194817\\\n",
    "Student4 Name: Jesse Wiers\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3TPoO70b2iE"
   },
   "source": [
    "# **Instructions**\n",
    "\n",
    "1. For both parts of the final project, students are expected to prepare a report. The report should include answers to all questions, written details on implementation approaches, the analysis of the results for different settings and visualizations to illustrate experiments with and performance of your implementation. Grading will primarily be based on the report (i.e. it should be self-contained as much as possible). If the report contains any faulty results or ambiguities, the TA's can take a look at your code to find out what happened.\n",
    "\n",
    "2. Do not just provide numbers without explanation, remember to follow the general guidelines and discuss different settings to show you understand the material and the processes at work.\n",
    "\n",
    "3. This part of the Final Lab contains an *optional* bonus challenge. See section 3 below.\n",
    "\n",
    "**Hint:** Having visual elements such as charts, graphs and plots are always useful for everyone. Keep this in mind while writing your reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY6wdmc299h1"
   },
   "source": [
    "# **General Guidelines**\n",
    "1. Aim:\n",
    " - Able to understand Image Recognition/Classification using Convolutional Neural Networks.\n",
    " - Get familiar with PyTorch, widely used Deep Learning framework\n",
    "2. Prerequisite:\n",
    " - Familiarity with Python and relevant packages.\n",
    " - Know the basics of feature descriptors (SIFT, HoG) and machine learning tools (K-means, SVM and etc.).\n",
    "3. Guidelines:\n",
    "    Students should work on the assignments in their assignment group for **two** weeks.\n",
    "\n",
    "    Any questions regarding the assignment content can be discussed on Piazza.\n",
    "    \n",
    "    Your source code and report must be handed in together in a zip file (**ID1_ID2_ID3_part2.zip**) before the deadline. Make sure your report follows these guidelines:\n",
    "    - *The maximum number of pages for this part is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n",
    "    - *Follow the given instructions and answer all given questions. Briefly describe what you implemented.*\n",
    "    - *Show you understand the algorithms and implementations: explain why certain settings produce certain results. When constructing graphs, tables and other figures, make your figures as informative as possible (choose relevant sample sizes, axes, etc.), to illustrate your arguments*\n",
    "    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n",
    "\n",
    "4. The report should be handed in in **PDF-format**. Your code should be handed in in **.ipynb format**. This does not mean you have to make your project in a notebook, it just means it **should be submitted as a notebook**. Be sure to test whether all your functionality works as expected when ran in a notebook. If you use a Conda environment, be sure to include it in your submission.\n",
    "\n",
    "5. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "6. **Plagiarism note**:\n",
    "Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations.\n",
    "This includes the use of ChatGPT and other generative AI tools.\n",
    "\n",
    "<!-- ### PyTorch versions\n",
    "we assume that you are using latest PyTorch version(>=1.4)\n",
    "\n",
    "### PyTorch Tutorial & Docs\n",
    "This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n",
    "\n",
    "**Anaconda Environment** We recommend installing \\textit{anaconda} for configuring \\textit{python} package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n",
    "\n",
    "**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n",
    "\n",
    "**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***:} What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n",
    "You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2EILNe3JR1b"
   },
   "source": [
    "#  **Introduction**\n",
    "\n",
    "This part of the assignment makes use of Convolutional Neural Networks (CNN's). The previous part makes use of hand-crafted features like SIFT to represent images, then trains a classifier on top of them. In this way, learning is a two-step procedure with image representation and learning. The method used here instead *learns* the features jointly with the classification. Training CNNs roughly consists of three parts:\n",
    "1. Creating the network architecture\n",
    "\n",
    "2. Preprocessing the data\n",
    "\n",
    "3. Feeding the data to the network, and updating the parameters.\n",
    "\n",
    "Please follow the instructions and finish the below tasks. (**Note:**  you are allowed to change the provided code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXQBnhT499h2"
   },
   "source": [
    "# **1: Image Classifiation on CIFAR-100**\n",
    "### 1.1 Install pytorch and run the given code\n",
    "\n",
    "First of all, you need to install PyTorch and relevant packages. In this part, we will use [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) as the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47K1Kzph99h3"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# referenced code: https://pytorch.org/tutorials/\n",
    "# referenced code: http://cs231n.stanford.edu/\n",
    "# referenced code: https://cs.stanford.edu/~acoates/stl10/\n",
    "######################################################\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AU03smp99h4"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('apple', 'aquarium_fish', 'baby','bear', 'beaver','bed','bee','beetle','bicycle','bottle', 'bowl','boy','bridge', 'bus','butterfly', 'camel','can','castle','caterpillar','cattle',\n",
    " 'chair','chimpanzee','clock','cloud', 'cockroach','couch', 'cra','crocodile', 'cup','dinosaur','dolphin', 'elephant','flatfish', 'forest', 'fox','girl', 'hamster', 'house','kangaroo','keyboard',\n",
    "'lamp', 'lawn_mower', 'leopard', 'lion','lizard','lobster', 'man','maple_tree','motorcycle', 'mountain', 'mouse','mushroom','oak_tree', 'orange','orchid', 'otter', 'palm_tree','pear', 'pickup_truck','pine_tree',\n",
    "'plain', 'plate', 'poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea', 'seal', 'shark','shrew', 'skunk','skyscraper', 'snail','snake','spider',\n",
    "'squirrel', 'streetcar', 'sunflower','sweet_pepper', 'table','tank','telephone', 'television', 'tiger','tractor','train','trout', 'tulip', 'turtle','wardrobe', 'whale', 'willow_tree','wolf', 'woman','worm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvNbVoMh99h5"
   },
   "source": [
    "####  **` Q1.1: Test dataloader and show the images of each class of CIFAR-100 (3-pts)`**  \n",
    "You need to run and modify the given code and **show** the example images of CIFAR-100 and **describe** the classes and images of CIFAR-100. (Please visualize at least one picture for the classes of labels from 0 to 4.) (3-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iPwTYDB99h7"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ6OkCV9LIM9"
   },
   "source": [
    "### 1.2 Architecture understanding\n",
    "\n",
    "In this section, we provide two wrapped classes of architectures defined by *nn.Module*. One is an ordinary two-layer network (*TwolayerNet*) with fully connected layers and ReLu, and the other is a Convolutional Network (*ConvNet*) utilizing the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XIW0peRVGcK"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6a1z2vK99h7"
   },
   "source": [
    "####  **`Q1.2: Architecture understanding. Implement architecture of TwolayerNet and ConvNet (4-pts).`**\n",
    "1. Complement the architecture of the *TwolayerNet* class, and complement the architecture of *ConvNet* class using the structure of LeNet-5. (2-*pts*)\n",
    "2. Since you need to feed color images into these two networks, what's the kernel size of the first convolutional layer in *ConvNet*? and how many trainable parameters are there in \"F6\" layer (given the calculation process)? (2-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5k5WMCCVGcL"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCP1tS7lL8hF"
   },
   "source": [
    "### 1.3 Preparation of training\n",
    "\n",
    "In above section, we use the *CIFAR-100* dataset class from *torchvision.utils* provided by PyTorch. Whereas in most cases, you need to prepare the dataset yourself. One of the ways is to create a *dataset* class yourself and then use the *DataLoader* to make it iterable. After preparing the training and testing data, you also need to define the transform function for data augmentation and optimizer for parameter updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyc8Ksd99h-"
   },
   "source": [
    "####  **` Q1.3: Preparation of training. Create Dataloader yourself and define the transform function and optimizer.(8-pts)`**  \n",
    "1. Complement the *CIFAR100\\_loader* (2-pts)\n",
    "2. Complement *Transform* function and *Optimizer* (2-pts)\n",
    "3. Train the *TwolayerNet* and *ConvNet* with *CIFAR100\\_loader*, *Transform* and *Optimizer* you implemented and compare the results (4-pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkImoAsU99h-"
   },
   "source": [
    "##### *` Complement  CIFAR100_loader()(2-pts)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYKtNWW_99h_"
   },
   "source": [
    "##### *` Complement Transform function and Optimizer (2-pts)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYpJN5Cb99h_"
   },
   "source": [
    "##### *` Train the TwolayerNet and ConvNet with CIFAR100_loader, transform and optimizer you implemented and compare the results (4-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-c5Q5-R2VGcM"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQhUzRuQOOSm"
   },
   "source": [
    "### 1.4 Setting up the hyperparameters\n",
    "\n",
    "Some parameters must be set properly before the training of CNNs. These parameters shape the training procedure. They determine how many images are to be processed at each step, how much the weights of the network will be updated, how many iterations the network will run until convergence. These parameters are called hyperparameters in the machine learning literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhtGVNHIQ8kT"
   },
   "source": [
    "####  **` Q1.4: Setting up the hyperparameters (10-pts)`**  \n",
    "\n",
    "1. Play with ConvNet and TwolayerNet yourself, set up the hyperparameters, and reach the accuracy as high as you can.\n",
    "You can modify the *train*,  *Dataloader*, *transform* and *Optimizer* function as you like.\n",
    "2. You can also modify the architectures of these two Nets.\n",
    "\n",
    " *Let's add 2 more layers in TwolayerNet and ConvNet, and show the results. (You can decide the size of these layers and where to add them.) Will you get higher performances? explain why.*\n",
    "3.  Show the final results and described what you've done to improve the results. Describe and explain the influence of hyperparameters among *TwolayerNet* and *ConvNet*.\n",
    "4. Compare and explain the differences of these two networks regarding the architecture, performances, and learning rates.\n",
    "\n",
    "**Hint:** You can adjust the following parameters and other parameters not listed as you like: *Learning rate, Batch size, Number of epochs, Optimizer, Transform function, Weight decay etc.* You can also change the structure a bit, for instance, adding Batch Normalization layers.\n",
    "\n",
    "**Note:** Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Puiytn99h_"
   },
   "source": [
    "#### *`Play with convNet and TwolayerNet, set up the hyperparameters and reach the accuracy as high as you can`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqQ6-Krp99iA"
   },
   "source": [
    "#### *` test the accuracy of ConvNet `*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlAIR45199iA"
   },
   "source": [
    "#### *`test the accuracy of TwolayerNet`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_d1BlXRVGcN"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XX8c2fb99iA"
   },
   "source": [
    "# **2:  Finetuning the ConvNet**\n",
    "### 2.1 STL-10 DATASET\n",
    "The above networks are trained on CIFAR-100, which\n",
    "contains the images of 100 different object categories, each of which has $32\\times32 \\times3$ dimensions.\n",
    "The dataset we use throughout the following part is a subset of [STL-10](https://cs.stanford.edu/~acoates/stl10/)\n",
    "with higher resolution and different object classes. So, there is a discrepancy between the previous dataset (CIFAR-100) and the new dataset (STL-10). One solution would be to train the whole network from scratch. However, the number of parameters is too large to be trained properly with such few images. Another way is to use the pre-trained network (on CIFAR-100) and then finetune the network on the new dataset (STL-10) (*e.g.*, use the same architectures in all layers except the output layer, as the number of output classes changes (from 100 to 5)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW0p0R6d99iA"
   },
   "source": [
    "#### **`Q2.1 Create the STL10_Dataset (5-pts)`**\n",
    "In this part, download STL-10 and extract 5 classes from STL-10 training dataset. The the labels of images will be defined as:\n",
    "\n",
    "`{1:'car', 2:'deer', 3:'horse', 4:'monkey', 5:'truck'}`\n",
    "\n",
    "Extract the above 5 classes of images from STL-10. Complement the *`STL10_Dataset`* class and match each class with the label accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO7KE5KdVGcN"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLyFVEWw3Ge"
   },
   "source": [
    "### 2.2 Fine-tuning ConvNet\n",
    "You should load the pre-trained parameters and modify the output layer of pre-trained ConvNet from 100 to 5. You can either load the pre-trained parameters and then modify the output layer, or change the output layer first and then load the matched pre-trained parameters. The examples can be found [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC3ipaYR99iB"
   },
   "source": [
    "#### **`Q2.2  Finetuning from ConvNet (10-pts)`**\n",
    "1. Load the pre-trained parameters (pretrained on CIFAR-100) and modify the ConvNet. (5-pts)\n",
    "2. Train the model and show the results (settings of hyperparameters, accuracy, learning curve). (5-pts)\n",
    "\n",
    "**Hint**:  Once the network is trained, it is a good practice to understand the feature space by visualization techniques. There are several techniques to visualize the feature space. [**t-sne**](https://lvdmaaten.github.io/tsne/) is a dimensionality reduction method which can help you better understand the feature learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xIbZsYkVGcO"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDUT9jtY99iB"
   },
   "source": [
    "#  **3. Bonus (optional)**\n",
    "Play with your code and try to get a higher accuracy on the test dataset (5 class from STL-10), as high as you can. The teams with the highest accuracy will get extra points, that will go on top of their average lab grade (weighed sum of all lab assignments) (your final grade can not exceed 10):\n",
    "\n",
    "**1st place:** *5%*\n",
    "\n",
    "**2nd and 3rd place:** *4%*\n",
    "\n",
    "**4th and 5th place:** *3%*\n",
    "\n",
    "**6th and 7th place:** *2%*\n",
    "\n",
    "**8th-10th place:** *1%*.\n",
    "\n",
    "You can adjust the hyperparameters and changing structures. Your strategies should be described and explained in your report.\n",
    "\n",
    "**Note:** Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers.\n",
    "\n",
    "**Note:** The only data you can use is from CIFAR-100 and SLT-10.\n",
    "\n",
    "**Hints**:\n",
    "*   Data augmentation\n",
    "*   Grid Search\n",
    "*   Freezing early layers\n",
    "*   Modifying Architecture\n",
    "*   Modifying hyperparameters, *etc*.\n",
    "*   [Other advice](https://cs231n.github.io/transfer-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1YsjfP199iB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grbC046Fgcs0"
   },
   "source": [
    " # Individual Contribution Report *(Mandatory)*\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I5rcHbegcs0"
   },
   "source": [
    "Name: Despoina Touska\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN9rG4a60Hv2"
   },
   "source": [
    "Name: Ekin Fergan\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nosUhs70H0E"
   },
   "source": [
    "Name: Gregory Hok Tjoan Go\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Jesse Wiers\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RctQ8Z1Cgcs0"
   },
   "source": [
    " # -End of Notebook-"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
