{"cells":[{"cell_type":"markdown","id":"9e481aef","metadata":{"id":"9e481aef"},"source":["<center> <img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=20% > </center>\n","\n","# <center> Assignment 1: Photometric Stereo & Colour </center>\n","<center> Computer Vision 1 University of Amsterdam </center>\n","<center> Due 23:59PM, September 16, 2023 (Amsterdam time) </center>\n","<center> <b>TA's: Ozzy Ülger, Egoitz Gonzalez, Sina Taslimi</b></center>\n","    \n","***\n","\n","Student1 ID: \\\n","Student1 Name:\n","\n","Student2 ID: \\\n","Student2 Name:\n","\n","Student3 ID: \\\n","Student3 Name:\n","\n","( Student4 ID: \\\n","Student4 Name: )\n"]},{"cell_type":"markdown","id":"cf780ad4","metadata":{"id":"cf780ad4"},"source":["# General guidelines\n"]},{"cell_type":"markdown","id":"b6bf68aa","metadata":{"id":"b6bf68aa"},"source":["Your code and discussion must be handed in this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. Please also fill out your names and ID's above.\n","\n","For full credit, make sure your notebook follows these guidelines:\n","- It is mandatory to use the Python environment provided with the assignment; the environment specifies the package versions that have to be used to prevent the use of particular functions. Using different packages versions may lead to grade deduction. In the 'Environment Setup' below you can check whether your environment is set up correctly.\n","- To install the environment with the right package versions, use the following command in your terminal: ```python\n","conda env create --file=environment.yaml```, then activate the environment using the command ```python conda activate cv1```.\n","- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n","- Answer all given questions and sub-questions.\n","- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make the notebook self-contained and complete.\n","- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n","\n","__Note:__ A more complete overview of the lab requirements can be found in the Course Manual on Canvas\n","\n","Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n","Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of generative tools such as ChatGPT.\n","\n","**ENSURE THAT YOU SAVE ALL RESULTS / ANSWERS ON THE QUESTIONS (EVEN IF YOU RE-USE SOME CODE).**"]},{"cell_type":"markdown","id":"60fe796b","metadata":{"id":"60fe796b"},"source":["# 0 Environment Setup"]},{"cell_type":"code","execution_count":null,"id":"e74bbda5","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:29:50.710673Z","start_time":"2023-04-11T08:29:50.177009Z"},"id":"e74bbda5"},"outputs":[],"source":["# environment and libraries\n","import os\n","import glob\n","import numpy as np\n","import random\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from mpl_toolkits.mplot3d import Axes3D   # necessary in part 4."]},{"cell_type":"code","execution_count":null,"id":"4c8f202b","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:07:21.002248Z","start_time":"2023-04-11T08:07:20.991533Z"},"id":"4c8f202b"},"outputs":[],"source":["# Make sure you're using the provided environment!\n","assert cv2.__version__ == \"3.4.2\", \"You're not using the provided Python environment!\"\n","assert np.__version__ == \"1.19.5\", \"You're not using the provided Python environment!\"\n","assert matplotlib.__version__ == \"3.3.4\", \"You're not using the provided Python environment!\"\n","# Proceed to the next cell if you don't get any error."]},{"cell_type":"markdown","id":"64a206a7","metadata":{"id":"64a206a7"},"source":["# 1 Colour Spaces *(13 pts)*"]},{"cell_type":"markdown","id":"ba6a562c","metadata":{"id":"ba6a562c"},"source":["In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space"]},{"cell_type":"markdown","id":"5b85aa4e","metadata":{"id":"5b85aa4e"},"source":["### 1.1 RGB Colour Model (2 pts)\n","\n","Why do we use a RGB colour model as the basis of our digital cameras and photography? How does a standard digital camera capture the full RGB colour image?\n"]},{"cell_type":"markdown","id":"5f95c6b1","metadata":{"id":"5f95c6b1"},"source":["*Write your answer here*\n"]},{"cell_type":"markdown","id":"58e3f640","metadata":{"id":"58e3f640"},"source":["### 1.2 Colour Space Conversion *(8 pts)*\n","\n","\n"]},{"cell_type":"markdown","id":"a9a3677d","metadata":{"id":"a9a3677d"},"source":["Create a function to convert an RGB image into the following colour spaces by using the template code you are provided ConvertColourSpace() below and other sub-functions. Visualize the new image and its channels separately in the same figure. That is, for example, in the case of HSV colour space, you need to visualize the converted HSV image, and its Hue, Saturation and Value channels separately (4 images, 1 figure).\n","\n","__Grayscale__\n","\n","Convert the RGB image into grayscale by using 3 different methods mentioned in\n","https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\n","In addition, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 in the same figure.\n","\n","__Opponent Colour Space__\n","\n","$\\begin{pmatrix}\n","O_1 \\\\\n","O_2 \\\\\n","O_3\n","\\end{pmatrix}$ = $\\begin{pmatrix}\n","\\frac{R-G}{\\sqrt{2}} \\\\\n","\\frac{R+G-2B}{\\sqrt{6}} \\\\\n","\\frac{R+G+B}{\\sqrt{3}}\n","\\end{pmatrix}$\n","\n","__Normalized RGB (rgb) Colour Space__\n","\n","$\\begin{pmatrix}\n","r \\\\\n","g \\\\\n","b\n","\\end{pmatrix}$ = $\\begin{pmatrix}\n","\\frac{R}{R+G+B} \\\\\n","\\frac{G}{R+G+B} \\\\\n","\\frac{B}{R+G+B}\n","\\end{pmatrix}$\n","\n","__HSV Colour Space__\n","\n","Convert the RGB image into HSV Colour Space. Use OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n","\n","__YCbCr Colour Space__\n","\n","Convert the RGB image into YCbCr Colour Space. Use OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Note, you need to arrange the channels in $Y, C_b$ and $C_r$ order.\n","\n","__HINT 1__\n","\n","Ensure you understand the datatypes and ranges that the python conversion and image displaying functions require. This usually is [0, 1] for float datatype or [0, 255] for integer datatype. You may have to explicitely change the datatype.\n","\n","__HINT 2__\n","\n","Think about how to visualise for instance H, S, V channels. Do this in a manner that is meaningfull, e.g. visualize each of the H, S, V channels in a RGB manner.  "]},{"cell_type":"code","execution_count":null,"id":"f59aa7bb","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:24:18.003763Z","start_time":"2023-04-11T08:24:17.998876Z"},"id":"f59aa7bb"},"outputs":[],"source":["def rgb2grays(input_image, method='opencv'):\n","    # converts an RGB into grayscale by using 4 different methods\n","    # YOUR CODE HERE\n","\n","    # lightness method\n","    # YOUR CODE HERE\n","\n","    # average method\n","    # YOUR CODE HERE\n","\n","    # luminosity method\n","    # YOUR CODE HERE\n","\n","    # built-in opencv function\n","    # YOUR CODE HERE\n","\n","    return new_image\n","\n","\n","def rgb2opponent(input_image):\n","    # converts an RGB image into opponent colour space\n","    # YOUR CODE HERE\n","\n","    return new_image\n","\n","def rgb2normedrgb(input_image):\n","    # converts an RGB image into normalized RGB colour space\n","    # YOUR CODE HERE\n","\n","    return new_image\n"]},{"cell_type":"code","execution_count":null,"id":"83e9c2d0","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:24:22.505914Z","start_time":"2023-04-11T08:24:22.493092Z"},"id":"83e9c2d0"},"outputs":[],"source":["def visualize(input_image, new_image, colourpace='rgb'):\n","    '''\n","     Visualize the new image and its channels separately in the same figure.\n","     That is, for example, in the case of HSV colour space, you need to\n","     visualize the converted HSV image, and its Hue, Saturation and Value\n","     channels separately (4 images, 1 figure).\n","    '''\n","\n","    # YOUR CODE HERE\n","\n","    raise NotImplementedError"]},{"cell_type":"code","execution_count":null,"id":"5f398f48","metadata":{"id":"5f398f48"},"outputs":[],"source":["def ConvertColourSpace(input_image, colourspace):\n","    '''\n","    Converts an RGB image into a specified color space, visualizes the\n","    color channels and returns the image in its new color space.\n","\n","    Colorspace options:\n","      opponent\n","      rgb -> for normalized RGB\n","      hsv\n","      ycbcr\n","      gray\n","\n","    P.S: Do not forget the visualization part!\n","    '''\n","\n","    # Convert the image into double precision for conversions\n","    input_image = input_image.astype(np.float32)\n","\n","    if colourspace.lower() == 'opponent':\n","        # fill in the rgb2opponent function\n","        new_image = rgb2opponent(input_image)\n","\n","    elif colourspace.lower() == 'rgb':\n","        # fill in the rgb2normedrgb function\n","        new_image = rgb2normedrgb(input_image)\n","\n","    elif colourspace.lower() == 'hsv':\n","        # use built-in function from opencv\n","        # YOUR CODE HERE\n","        pass\n","\n","    elif colourspace.lower() == 'ycbcr':\n","        # use built-in function from opencv\n","        # YOUR CODE HERE\n","        pass\n","\n","    elif colourspace.lower() == 'gray':\n","        # fill in the rgb2grays function\n","        new_image = rgb2grays(input_image)\n","\n","    else:\n","        print('Error: Unknown colorspace type [%s]...' % colourspace)\n","        new_image = input_image\n","\n","    return new_image\n"]},{"cell_type":"markdown","id":"24d74a84","metadata":{"id":"24d74a84"},"source":["### 1.3 Colour Space Properties (2 pts)\n","\n","Explain each of the above 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."]},{"cell_type":"markdown","id":"8ff98147","metadata":{"id":"8ff98147"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"9257ed32","metadata":{"id":"9257ed32"},"source":["### 1.4 More on Colour Spaces (1 pt)\n","\n","Find one more colour space from the literature, briefly explain its properties and give a use case."]},{"cell_type":"markdown","id":"cd2c87cb","metadata":{"id":"cd2c87cb"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"22b8bc0c","metadata":{"id":"22b8bc0c"},"source":["# 2 Colour Constancy *(15 pts)*"]},{"cell_type":"markdown","id":"311de675","metadata":{"id":"311de675"},"source":["Colour constancy is the ability to perceive colors of objects, invariant to the colour of the light source. The aim for colour constancy algorithms is first to estimate the illuminant of the light source, and then correct the image so that the corrected image appears to be taken under a canonical (white) light source. The task of the automatic white balance (AWB) is to do the same in digital cameras so that the images taken by a digital camera look as natural as possible.\n","\n","In this part of the assignment, you will implement the most famous colour constancy algorithm; *Grey-World Algorithm*. The algorithm assumes that, under a white light source, the average colour in a scene should be achromatic (grey, [128, 128, 128]).\n","\n","Specific information on the algorithm can be found on:\n","https://en.wikipedia.org/wiki/Color_normalization#Grey_world\n","\n"]},{"cell_type":"markdown","id":"dceff985","metadata":{"id":"dceff985"},"source":["### 2.1 Grey-World algorithm (8 pts)\n","\n","Complete the function to apply colour correction to an RGB image by using Grey-World algorithm. Display the original image and the colour corrected one on the same figure. Use awb.jpg image (provided in the downloaded zip-file) to test your algorithm. In the end, you should see that the reddish colour cast on the image is removed and it looks more natural.\n","\n","  ***Note:*** You do not need to apply any pre or post processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*.\n","   "]},{"cell_type":"code","execution_count":null,"id":"52e1b7ab","metadata":{"id":"52e1b7ab"},"outputs":[],"source":["def grey_world(awb_img):\n","\n","    # YOUR CODE HERE\n","\n","    return colorCorrectedImg"]},{"cell_type":"markdown","id":"aedb822a","metadata":{"id":"aedb822a"},"source":["### 2.2 Limits of Grey-World (2 pts)\n","\n","Give an example case for Grey-World Algorithm on where it might fail. Include your reasoning.\n"]},{"cell_type":"markdown","id":"5a07ba6e","metadata":{"id":"5a07ba6e"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"880a1dca","metadata":{"id":"880a1dca"},"source":["### 2.3 Other Constancy algorithms (5 pts)\n","\n","Find out one more colour constancy algorithm from the literature and explain it briefly.\n"]},{"cell_type":"markdown","id":"b3806857","metadata":{"id":"b3806857"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"e982e316","metadata":{"id":"e982e316"},"source":["# 3 Intrinsic Image Decomposition *(12 pts)*"]},{"cell_type":"markdown","id":"f40cba7e","metadata":{"id":"f40cba7e"},"source":["\n","Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumptions of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n","\n","$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n","\n","In this part of the assignment, you will experiment with intrinsic image components to perform a particular computational photography applications: material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n","\n","<a name=\"cite_note-1\"></a><small>1. [^](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n","\n","<a name=\"cite_note-2\"></a><small>2. [^](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n","\n"]},{"cell_type":"markdown","id":"97f6c43b","metadata":{"id":"97f6c43b"},"source":["### 3.1 Image decomposition *(2 pts)*\n","\n","In what other components can an image be decomposed other than albedo and shading? Give an example and explain the concepts in your answer."]},{"cell_type":"markdown","id":"59625c3c","metadata":{"id":"59625c3c"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"4b991c5c","metadata":{"id":"4b991c5c"},"source":["### 3.2 Synthetic data *(2 pts)*\n","\n","If you check the literature, you will see that almost all intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that?"]},{"cell_type":"markdown","id":"03fec92b","metadata":{"id":"03fec92b"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"8ebc5a59","metadata":{"id":"8ebc5a59"},"source":["### 3.3. Reconstructing *(4 pts)*\n","\n","Pick a favorite object from the synthetic intrinsic image dataset and store it's original image, shading and reflectance in a new folder in *'./images'*, e.g. *./images/intrinsic_images/* (or choose your own folder and change the path in the code). Show that you can actually reconstruct the original PNG image from its intrinsics using the albedo and shading. In the end, your script should output a figure displaying the original image, its intrinsic images and the reconstructed one. Complete the code for function **iid_image_formation()**.\n","\n","The auxiliary functions are for opening and visualizing the original and intrinsic images. In your submission, only supply the images of the object you picked."]},{"cell_type":"code","execution_count":null,"id":"bc3a9762","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:27:14.946724Z","start_time":"2023-04-11T08:27:14.938008Z"},"id":"bc3a9762"},"outputs":[],"source":["def iid_image_formation(albedo_img, shading_img):\n","\n","    # YOUR CODE HERE\n","\n","    return idd_img"]},{"cell_type":"code","execution_count":null,"id":"3f1913cf","metadata":{"id":"3f1913cf"},"outputs":[],"source":["## Replace the image name with a valid image\n","img_path = './images/intrinsic_images/'\n","object_name = ''\n","\n","## Read with opencv\n","picked_object = cv2.imread(img_path + object_name + '.png')\n","if picked_object is None:\n","    print('no image found')\n","else:\n","    print(object_name + ':', picked_object.shape)\n","\n","objectShading = cv2.imread(img_path + object_name + '_shad.png')\n","if objectShading is None:\n","    print('no image found')\n","else:\n","    print('objectShading:', objectShading.shape)\n","\n","objectAlbedo = cv2.imread(img_path + object_name + '_refl.png')\n","if objectAlbedo is None:\n","    print('no image found')\n","else:\n","    print('objectAlbedo:', objectAlbedo.shape)\n","\n","\n","picked_object_IDD = iid_image_formation(objectAlbedo, objectShading)\n","visualize((picked_object/picked_object.max()*255.).astype(int),\n","          (picked_object_IDD/picked_object_IDD.max()*255.).astype(int))\n","\n"]},{"cell_type":"markdown","id":"fbed5e37","metadata":{"id":"fbed5e37"},"source":["### 3.4 Recoloring *(4 pts)*\n"]},{"cell_type":"markdown","id":"f06955f1","metadata":{"id":"f06955f1"},"source":["Manipulating colours in photographs is an important problem with many applications in computer vision. Since the aim for recolouring algorithms is just to manipulate colours, better results can be obtained for such a task if the albedo image is available as it is independent of confounding illumination effects.\n","\n","Assume that you are given the PNG image and you have access to its\n","intrinsic albedo and shading images.\n","1. Find out the true material colour of the object you picked in RGB space (which is uniform in this case).\n","2. Recolour the object's image with pure green (0, 255, 0). Display the original object image and the recoloured version on the same figure. Complete the code for function **recolouring()**."]},{"cell_type":"markdown","id":"129789ed","metadata":{"id":"129789ed"},"source":["3. Although you have recoloured the object with pure green, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason."]},{"cell_type":"code","execution_count":null,"id":"465562c9","metadata":{"id":"465562c9"},"outputs":[],"source":["def recolouring(albedo_img, shading_img):\n","\n","    # YOUR CODE HERE\n","\n","    return recoloured_image"]},{"cell_type":"markdown","id":"da57fc68","metadata":{"id":"da57fc68"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"4802df63","metadata":{"id":"4802df63"},"source":["__Note:__  this was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."]},{"cell_type":"markdown","id":"bbe628dc","metadata":{"id":"bbe628dc"},"source":["# 4 Photometric Stereo *(60 pts)*\n","\n"]},{"cell_type":"markdown","id":"1ac58a17","metadata":{"id":"1ac58a17"},"source":["In this part of the assignment, you are going to implement the photometric stereo algorithm as described in Section 5.4 (Forsyth and Ponce, *Computer Vision: A Modern Approach*). The chapter snippet has been included in the assignment zip file.\n","\n","Following this instruction, you will edit and fill in your code in the procedures below **estimate_alb_nrm()**, **check_integrability()** and **construct_surface()**, that you can find in the codecells below (4.1, 4.2 and 4.3, respectively). The main script **photometric_stereo** (4.4) is provided for reference and should not be taken as is. Throughout the assignment, you will be asked to perform different trials and experiments which will require you to adjust the main code accordingly; this also shows how well you can cope with the materials.\n","\n","Include images of the results into your notebook at informative points. For 3D models, make sure to choose a viewpoint that makes the structure as clear as possible and/or feel free to take them from multiple viewpoints."]},{"cell_type":"markdown","id":"851b4671","metadata":{"id":"851b4671"},"source":["### 4.1 Estimating Albedo and Surface Normal *(15 pts)*\n","Let us start with the grayscale sphere model, which is located in the SphereGray5 folder. The folder contains 5 images of a sphere with a grayscale checker texture under similar lighting conditions as the one in the book. Your task is to estimate the surface reflectance (albedo) and surface normal of this model. The light source directions are encoded in the image file names.\n","\n","1. Complete the code for function **estimate_alb_nrm()** to estimate albedo and surface normal map for the SphereGray5 folder. What do you expect to see in albedo image and how is it different from your result?\n","2. In principle, what is the minimum number of images you need to estimate albedo and surface normal? Run the algorithm with more images by using SphereGray25, observe the differences in the results and report. You could try all images at once or a few at the time, in an incremental fashion. Choose a strategy and justify it by discussing your results.\n","3. What is the impact of shadows in photometric stereo? Explain the trick that is used in the text to deal with shadows. Remove that trick from your implementation and check your results. Is the trick necessary in the case of 5 images? how about 25 images?\n","\n","An answer box can be found below the code cells.\n","\n","**Hint**: To get the least-squares solution of a linear system, you can use **numpy.linalg.lstsq** function."]},{"cell_type":"code","execution_count":null,"id":"fbf8970a","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:30:00.131599Z","start_time":"2023-04-11T08:30:00.038210Z"},"id":"fbf8970a"},"outputs":[],"source":["# Helper functions to load the synthetic images\n","\n","def load_syn_images(image_dir='./images/photometrics_images/SphereGray5/', channel=0):\n","    files = os.listdir(image_dir)\n","    nfiles = len(files)\n","\n","    image_stack = None\n","    V = 0\n","    Z = 0.5\n","\n","    for i in range(nfiles):\n","        # read input image\n","        im = cv2.imread(os.path.join(image_dir, files[i]))\n","        im = np.flip(im, axis=-1)\n","        im = im[:,:,channel]\n","\n","        # stack at third dimension\n","        if image_stack is None:\n","            h, w = im.shape\n","            print('Image size (H*W): %d*%d' %(h,w) )\n","            image_stack = np.zeros([h, w, nfiles], dtype=int)\n","            V = np.zeros([nfiles, 3], dtype=np.float64)\n","\n","        image_stack[:,:,i] = im\n","\n","        # read light direction from image name\n","        X = np.double(files[i][(files[i].find('_')+1):files[i].rfind('_')])\n","        Y = np.double(files[i][files[i].rfind('_')+1:files[i].rfind('.png')])\n","        V[i, :] = [-X, Y, Z]\n","\n","    # normalization\n","    image_stack = np.double(image_stack)\n","    min_val = np.min(image_stack)\n","    max_val = np.max(image_stack)\n","    image_stack = (image_stack - min_val) / (max_val - min_val) if max_val!=min_val else np.zeros(image_stack.shape)  # avoid fail when image_stack==0\n","    normV = np.tile(np.sqrt(np.sum(V ** 2, axis=1, keepdims=True)), (1, V.shape[1]))\n","    scriptV = V / normV\n","\n","    return image_stack, scriptV"]},{"cell_type":"code","execution_count":null,"id":"2a25b4cf","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:30:55.483238Z","start_time":"2023-04-11T08:30:55.446538Z"},"id":"2a25b4cf"},"outputs":[],"source":["## Helper function for showing results.\n","\n","def show_results(albedo, normals, height_map=None, SE=None):\n","    # Stride in the plot, you may want to adjust it to different images\n","    stride = 1\n","\n","    if albedo is not None:\n","        # showing albedo map\n","        fig = plt.figure()\n","        #albedo_max = albedo.max()\n","        albedo_max = 1\n","        albedo = albedo / albedo_max\n","        print(albedo.shape)\n","        plt.imshow(albedo, cmap=\"gray\")\n","        plt.show()\n","\n","    # showing normals as three separate channels\n","    figure = plt.figure()\n","    ax1 = figure.add_subplot(131)\n","    ax1.imshow(normals[..., 0])\n","    ax2 = figure.add_subplot(132)\n","    ax2.imshow(normals[..., 1])\n","    ax3 = figure.add_subplot(133)\n","    ax3.imshow(normals[..., 2])\n","    plt.show()\n","\n","    # meshgrid\n","    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], stride),\n","    np.arange(0,np.shape(normals)[1], stride),\n","    np.arange(1))\n","    X = X[..., 0]\n","    Y = Y[..., 0]\n","\n","    '''\n","    =============\n","    You could further inspect the shape of the objects and normal directions by using plt.quiver() function.\n","    =============\n","    '''\n","\n","    if height_map is not None:\n","      # plotting model geometry\n","      H = height_map[::stride,::stride]\n","      fig = plt.figure()\n","      ax = fig.gca(projection='3d')\n","      ax.plot_surface(X,Y, H.T)\n","      plt.show()\n","\n","    if SE is not None:\n","      # plotting the SE\n","      H = SE[::stride,::stride]\n","      fig = plt.figure()\n","      ax = fig.gca(projection='3d')\n","      ax.plot_surface(X,Y, H.T)\n","      plt.show()"]},{"cell_type":"code","execution_count":null,"id":"484ee70a","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:31:44.451373Z","start_time":"2023-04-11T08:31:44.416212Z"},"id":"484ee70a"},"outputs":[],"source":["def estimate_alb_nrm(image_stack, scriptV, shadow_trick=True):\n","    '''\n","    Compute the gradient of the surface\n","    INPUT:\n","        - image_stack : the images of the desired surface stacked up on the 3rd dimension\n","        - scriptV : matrix V (in the algorithm) of source and camera information\n","        - shadow_trick: (true/false) whether or not to use shadow trick in solving linear equations\n","    OUTPUT:\n","        - albedo : the surface albedo\n","        - normal : the surface normal\n","    '''\n","\n","    h, w, _ = image_stack.shape\n","\n","    # create arrays for\n","    # albedo (1 channel)\n","    # normal (3 channels)\n","    albedo = np.zeros([h, w])\n","    normal = np.zeros([h, w, 3])\n","\n","    # ================\n","    # YOUR CODE HERE\n","    #\n","    # for each point in the image array\n","    #     stack image values into a vector i\n","    #     construct the diagonal matrix scriptI\n","    #     solve scriptI * scriptV * g = scriptI * i to obtain g for this point\n","    #     albedo at this point is |g|\n","    #     normal at this point is g / |g|\n","    # ================\n","\n","    return albedo, normal\n"]},{"cell_type":"code","execution_count":null,"id":"cf3b09a8","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:35:01.818939Z","start_time":"2023-04-11T08:35:01.803451Z"},"id":"cf3b09a8"},"outputs":[],"source":["# load syn images\n","# YOUR CODE HERE\n","\n","# estimate_alb_nrm\n","# YOUR CODE HERE)\n","\n","# Show results\n","# YOUR CODE HERE"]},{"cell_type":"markdown","id":"10c03c0a","metadata":{"id":"10c03c0a"},"source":["\n","\n","*Write your answers here*"]},{"cell_type":"markdown","id":"b61d4868","metadata":{"id":"b61d4868"},"source":["### 4.2 Test of Integrability *(10 pts)*\n","\n","\n","\n","Before we can reconstruct the surface height map, it is required to compute the partial derivatives $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ (or *p* and *q* in the algorithm). The partial derivatives also give us a chance to double check our computation, namely the test of *integrability*."]},{"cell_type":"markdown","id":"8df77005","metadata":{"id":"8df77005"},"source":["#### 4.2.1. Compute the partial derivatives (p and q in the algorithm) by filling in your code into **check_integrability()** (5 pts)."]},{"cell_type":"code","execution_count":null,"id":"8cebe204","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:40:53.432777Z","start_time":"2023-04-11T08:40:53.418839Z"},"id":"8cebe204"},"outputs":[],"source":["def check_integrability(normals):\n","    '''\n","    Check the surface gradient is acceptable\n","    INPUTS:\n","        - normals: normal image\n","    OUTPUTS:\n","        - p : df / dx\n","        - q : df / dy\n","        - SE : Squared Errors of the 2 second derivatives\n","    '''\n","\n","    # initalization\n","    p = np.zeros(normals.shape[:2])\n","    q = np.zeros(normals.shape[:2])\n","    SE = np.zeros(normals.shape[:2])\n","\n","    # ================\n","    # YOUR CODE HERE\n","    # ================\n","    # Compute p and q, where\n","    # p measures value of df / dx\n","    # q measures value of df / dy\n","\n","    # change nan to 0\n","    p[p!=p] = 0\n","    q[q!=q] = 0\n","\n","    # ================\n","    # YOUR CODE HERE\n","    # approximate second derivate by neighbor difference\n","    # and compute the Squared Errors SE of the 2 second derivatives SE\n","    # ================\n","\n","    return p, q, SE\n","\n","# Use the normals that you found in the previous question\n","p, q, SE = check_integrability(normals)\n","print('SE:', SE.shape, SE.max())"]},{"cell_type":"markdown","id":"f50dce20","metadata":{"id":"f50dce20"},"source":["#### 4.2.2. Second derivatives (5 pts)\n","\n","Implement and compute the second derivatives according to the algorithm and perform the test of integrability by choosing a reasonable threshold. What could be the reasons for the errors? How does the test perform with different numbers of images used in the reconstruction process in 4.1?"]},{"cell_type":"markdown","id":"b7a3dd0b","metadata":{"id":"b7a3dd0b"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"459958e4","metadata":{"id":"459958e4"},"source":["### 4.3 Shape by Integration *(15 pts)*  \n","To reconstruct the surface height map, we need to continuously integrate the partial derivatives over a path. However, as we are working with discrete structures, you will be simply summing their values.\n","\n","The algorithm in the chapter presents a way to do the integration in column-major order, that is you start at the top-left corner and integrate along the first column, then go towards the right along each row. Yet, it is also noticed that it would be better to use many different paths and average so as to spread around the errors in the derivative estimates."]},{"cell_type":"markdown","id":"e4bf9a39","metadata":{"id":"e4bf9a39"},"source":["#### 4.3.1. Construct the surface height map (7 pts)\n","\n","Construct the surface height map using column-major order as described in the algorithm, then implement row-major path integration. Your code should go in **construct_surface()**.\n","\n","**Note**: By default, Numpy used row-major operations. So if you are unrolling an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to be column-major. Otherwise, if you are using the double for-loops without an unrolling operation, then this concern doesn’t apply.\n","\n","**Hint**: You could further inspect the shape of the objects and normal directions by using **matplotlib.pyplot.quiver** function. You will have to choose appropriate sub-sampling ratios for proper illustration. You code goes to the **show_results()** function in the code cell above."]},{"cell_type":"code","execution_count":null,"id":"6de49235","metadata":{"ExecuteTime":{"end_time":"2023-04-11T08:42:37.148187Z","start_time":"2023-04-11T08:42:37.060685Z"},"id":"6de49235"},"outputs":[],"source":["def construct_surface(p, q, path_type='column'):\n","    '''\n","    Construct the surface function represented as height_map\n","    INPUT:\n","       - p : measures value of df / dx\n","       - q : measures value of df / dy\n","       - path_type: type of path to construct height_map, either 'column',\n","         'row', or 'average'\n","    OUTPUT:\n","       - height_map: the reconstructed surface\n","    '''\n","\n","    h, w = p.shape\n","    height_map = np.zeros([h, w])\n","\n","    if path_type=='column':\n","        # ================\n","        # YOUR CODE HERE\n","        # ================\n","        # top left corner of height_map is zero\n","        # for each pixel in the left column of height_map\n","        #  height_value = previous_height_value + corresponding_q_value\n","\n","        # for each row\n","        #   for each element of the row except for leftmost\n","        #       height_value = previous_height_value + corresponding_p_value\n","        pass\n","\n","\n","    elif path_type=='row':\n","        # ================\n","        # YOUR CODE HERE\n","        # ================\n","        pass\n","\n","    elif path_type=='average':\n","        # ================\n","        # YOUR CODE HERE\n","        # ================\n","        pass\n","\n","    return height_map\n","\n","print('p:', p.shape, 'q:', q.shape, q.max())\n","height_map = construct_surface(p, q)\n","\n","show_results(albedo, norm, height_map)"]},{"cell_type":"markdown","id":"b9c2c70b","metadata":{"id":"b9c2c70b"},"source":["#### 4.3.2. What are the differences in the results of the two paths? (4 pts)\n"]},{"cell_type":"markdown","id":"52ac68e7","metadata":{"id":"52ac68e7"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"217e9c9d","metadata":{"id":"217e9c9d"},"source":["#### 4.3.3. Now, take the average of the results. (4 pts)\n","\n","Do you see any improvement compared to when using only one path? Are the construction results different with different numbers of images being used?"]},{"cell_type":"markdown","id":"93a4d0fe","metadata":{"id":"93a4d0fe"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"49d339fc","metadata":{"id":"49d339fc"},"source":["### 4.4 Experiments with different objects *(20 pts)*\n","In this part, you will try to run the photometric stereo algorithm in a various number of scenarios to see how well it can be generalized."]},{"cell_type":"markdown","id":"9e2f9f88","metadata":{"id":"9e2f9f88"},"source":["#### 4.4.1. Run the algorithm and show the results for the MonkeyGray model. *(5 pts)*"]},{"cell_type":"markdown","id":"d462e5b2","metadata":{"id":"d462e5b2"},"source":["##### 1.a Complete the code below, run the algorithm and show the results"]},{"cell_type":"code","execution_count":null,"id":"04c63dc0","metadata":{"id":"04c63dc0"},"outputs":[],"source":["def photometric_stereo(image_dir='./images/photometrics_images/MonkeyGray/'):\n","\n","    # obtain many images in a fixed view under different illumination\n","    print('Loading images...\\n')\n","    [image_stack, scriptV] = load_syn_images(image_dir)\n","    [h, w, n] = image_stack.shape\n","    print('Finish loading %d images.\\n' % n)\n","\n","    # compute the surface gradient from the stack of imgs and light source mat\n","    print('Computing surface albedo and normal map...\\n')\n","    # YOUR CODE HERE\n","\n","\n","    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n","    print('Integrability checking\\n')\n","    # YOUR CODE HERE\n","\n","    # threshold = XXX;\n","    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n","    SE[SE <= threshold] = float('nan') # for good visualization\n","\n","    # compute the surface height\n","    # YOUR CODE HERE\n","\n","    # show results\n","    # YOUR CODE HERE\n","\n","# Use the function\n","image_dir = 'images/photometrics_images/MonkeyGray/'\n","photometric_stereo(image_dir)"]},{"cell_type":"markdown","id":"8204b67e","metadata":{"id":"8204b67e"},"source":["##### 1.b Explain errors\n","\n","The albedo results of the monkey may comprise more albedo errors than in case of the sphere. Observe and describe the errors: Experiment with different cases and observe the errors that arise. (You don't need to calculate and give numerical errors.)\n","\n","What could be the reason for those errors? You may want to experiment with different numbers of images as you did in Question 1 to see the effects."]},{"cell_type":"markdown","id":"8307a824","metadata":{"id":"8307a824"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"0c169bb0","metadata":{"id":"0c169bb0"},"source":["##### 1.c What do you think could help solving these errors?"]},{"cell_type":"markdown","id":"f0647758","metadata":{"id":"f0647758"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"033dab58","metadata":{"id":"033dab58"},"source":["#### 4.4.2. Three-channel images. *(5 pts)*\n","\n","So far, we have assumed that albedos are 1-channel grayscale images and that input images are also 1-channel. To work with 3-channel images, a simple solution is to split the input image into separate channels and treat them individually. Yet, that would generate a small problem while constructing the surface normal map if a pixel value in a channel is zero."]},{"cell_type":"markdown","id":"fb45bb14","metadata":{"id":"fb45bb14"},"source":["\n","##### 2.a Update the implementation to work for 3-channel RGB inputs and test it with 2 models SphereColor and MonkeyColor.\n"]},{"cell_type":"code","execution_count":null,"id":"20be7384","metadata":{"id":"20be7384"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","source":["\n","##### 2.b Explain your changes and show your results."],"metadata":{"id":"Ew51ZQ-EaayM"},"id":"Ew51ZQ-EaayM"},{"cell_type":"markdown","id":"28b0945a","metadata":{"id":"28b0945a"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"1113e305","metadata":{"id":"1113e305"},"source":["##### 2.c Observe the problem in the constructed surface normal map and height map. Explain why a zero pixel could be a problem and propose a way to overcome that."]},{"cell_type":"markdown","id":"74b7dbe9","metadata":{"id":"74b7dbe9"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"0cf680b0","metadata":{"id":"0cf680b0"},"source":["\n","#### 4.4.3. Real world dataset A *(5 pts)*   \n","\n","Now, it's the time to try the algorithm on real-world datasets. For that purpose, we are going the use the Yale Face Database and the Apple dataset."]},{"cell_type":"markdown","id":"3e9b170d","metadata":{"id":"3e9b170d"},"source":["First the Yale Face Database.\n","\n","##### 3.a Run the algorithm for the Yale Face images: [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html). The Yale face data is included in the lab material.\n","\n","##### 3.b Observe and discuss the results for different integration paths.\n","\n","An answer box can be found below the code cells.\n","\n","**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n","$$i = Vg(x,y)$$"]},{"cell_type":"code","execution_count":null,"id":"cdb3d996","metadata":{"id":"cdb3d996"},"outputs":[],"source":["## help functions for loading Yale Face and Apple images\n","\n","def load_face_images(image_dir='./images/photometrics_images/yaleB02/'):\n","    num_images = 64\n","    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n","    ambient_image = cv2.imread(filename, -1)\n","    h, w = ambient_image.shape\n","\n","    # get list of all other image files\n","    import glob\n","    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n","    import random\n","    d = random.sample(d, num_images)\n","    filenames = [os.path.basename(x) for x in d]\n","\n","    ang = np.zeros([2, num_images])\n","    image_stack = np.zeros([h, w, num_images])\n","\n","    for j in range(num_images):\n","        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n","        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n","\n","\n","    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n","    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n","    z = np.sin(np.pi*ang[1,:]/180)\n","    scriptV = np.array([y,z,x]).transpose(1,0)\n","\n","    image_stack = np.double(image_stack)\n","    image_stack[image_stack<0] = 0\n","    min_val = np.min(image_stack)\n","    max_val = np.max(image_stack)\n","    image_stack = (image_stack - min_val) / (max_val - min_val) if max_val!=min_val else np.zeros(image_stack.shape)  # avoid fail when image_stack==0\n","\n","    return image_stack, scriptV\n","\n","\n","def load_apple_images(image_dir='./images/photometrics_images/Apple'):\n","    num_images = 99\n","    filename = os.path.join(image_dir, 'I_0000.png')\n","    try_image = cv2.imread(filename, -1)\n","    h, w = try_image[:,:,0].shape\n","\n","    # get list of all other image files\n","    import glob\n","    d = glob.glob(os.path.join(image_dir, 'I_00*.png'))\n","    import random\n","    d = random.sample(d, num_images)\n","    filenames = [os.path.basename(x) for x in d]\n","    filenames_idx = []\n","    for i in filenames:\n","        filenames_idx.append(int(i.split('_')[1].split('.')[0]))\n","\n","    ang = np.zeros([2, num_images])\n","    image_stack = np.zeros([h, w, num_images])\n","\n","    for j in range(num_images):\n","        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,0]\n","\n","    with open('./images/photometrics_images/Apple/light_directions_refined.txt') as file:\n","        lines = [line.split() for line in file]\n","        x, y, z = [], [], []\n","        for idx in filenames_idx:\n","            x.append(float(lines[idx][0]))\n","            y.append(float(lines[idx][1]))\n","            z.append(float(lines[idx][2]))\n","\n","    scriptV = np.array([y,z,x]).transpose(1,0)\n","\n","    image_stack = np.double(image_stack)\n","    image_stack[image_stack<0] = 0\n","    min_val = np.min(image_stack)\n","    max_val = np.max(image_stack)\n","    image_stack = (image_stack - min_val) / (max_val - min_val) if max_val!=min_val else np.zeros(image_stack.shape)  # avoid fail when image_stack==0\n","\n","    return image_stack, scriptV\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"93d20c0b","metadata":{"id":"93d20c0b"},"outputs":[],"source":["'''\n","For each of Face and Apple, you should:\n","- Load the images\n","- Compute the surface albedo and normal map\n","- Run the integrability check\n","- Find the number of outliers\n","- Compute the surface albedo and normal mape surface height\n","- Show the results\n","'''\n","\n","## Face\n","def photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/', path_type='average'):\n","    [image_stack, scriptV] = load_face_images(image_dir)\n","    [h, w, n] = image_stack.shape\n","    print('Finish loading %d images.\\n' % n)\n","    # Compute the surface albedo and normal map\n","    print('Computing surface albedo and normal map...\\n')\n","    # YOUR CODE HERE\n","    # albedo, normals = ...\n","\n","    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n","    print('Integrability checking')\n","    # YOUR CODE HERE\n","    # p, q, SE = ...\n","\n","    # YOUR CODE HERE\n","    # threshold = XXX;\n","    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n","    SE[SE <= threshold] = float('nan') # for good visualization\n","\n","    # YOUR CODE HERE\n","    # compute the surface height\n","    # height_map = ...\n","\n","    # show results\n","    # YOUR CODE HERE\n","\n","## Apple\n","def photometric_stereo_apple(image_dir='./images/photometrics_images/Apple/', path_type='average'):\n","    [image_stack, scriptV] = load_apple_images(image_dir)\n","    [h, w, n] = image_stack.shape\n","    print('Finish loading %d images.\\n' % n)\n","    # Compute the surface albedo and normal map\n","    print('Computing surface albedo and normal map...\\n')\n","    # YOUR CODE HERE\n","    # albedo, normals = ...\n","\n","    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n","    print('Integrability checking')\n","    # YOUR CODE HERE\n","    # p, q, SE = ...\n","\n","    # YOUR CODE HERE\n","    # threshold = XXX;\n","    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n","    SE[SE <= threshold] = float('nan') # for good visualization\n","\n","    # YOUR CODE HERE\n","    # compute the surface height\n","    # height_map = ...\n","\n","    # show results\n","    # YOUR CODE HERE"]},{"cell_type":"markdown","id":"1e5134b6","metadata":{"id":"1e5134b6"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"136ed58c","metadata":{"id":"136ed58c"},"source":["##### 3.c Discuss how the images violate the assumptions of the shape-from-shading methods. Remember to include specific input images to illustrate your points.\n"]},{"cell_type":"markdown","id":"e32f9353","metadata":{"id":"e32f9353"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"a0aa69d6","metadata":{"id":"a0aa69d6"},"source":["##### 3.d How would the results improve when the problematic images are all removed? Try it out and show the results in your notebook."]},{"cell_type":"markdown","id":"0b9e3adf","metadata":{"id":"0b9e3adf"},"source":["*Write your answer here*"]},{"cell_type":"markdown","id":"876ed243","metadata":{"id":"876ed243"},"source":["#### 4.4.4. Real world dataset B (5 pts).  "]},{"cell_type":"markdown","id":"950ff36b","metadata":{"id":"950ff36b"},"source":["And finally, the Apple dataset. Show your results on real-world 3-channel RGB inputs, contained in the \"Apple\" folder, taken from [this dataset](http://vision.ucsd.edu/~nalldrin/research/cvpr08/datasets/) from the University of California San Diego.   \n","\n","Observe and discuss the results for different integration paths. You may find difficulties in using this non-sythetic dataset. Try if filtering may help."]},{"cell_type":"code","execution_count":null,"id":"3b5bf059","metadata":{"id":"3b5bf059"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","id":"d232e456","metadata":{"id":"d232e456"},"source":["*Write your answers here*"]},{"cell_type":"markdown","id":"1338d438","metadata":{"id":"1338d438"},"source":["# X Individual Contribution Report *(Mandatory)*\n","\n","Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given.\n"]},{"cell_type":"markdown","id":"a66544c9","metadata":{"id":"a66544c9"},"source":["Name:\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","id":"5938a1ed","metadata":{"id":"5938a1ed"},"source":["Name:\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","id":"44c5a517","metadata":{"id":"44c5a517"},"source":["Name:\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing:"]},{"cell_type":"markdown","metadata":{"id":"OT95JG5E4qQH"},"source":["( Name:\n","\n","Contribution on research: \\\n","Contribution on programming: \\\n","Contribution on writing: )"],"id":"OT95JG5E4qQH"},{"cell_type":"markdown","id":"c3225dd1","metadata":{"id":"c3225dd1"},"source":["# -End of Notebook-"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":5}